[SPARK_HISTORY]
api_base_url = http://localhost:18080/api/v1
# ssl_path = /path/to/mycert.pem

[CRAWLER]
# Time between pulling of new completed runs from Spark History
sleep_seconds = 300

# When YARN is used as Spark master, if a run attempt fails it might be
# restarted with the same app id but a new attempt id.
# After an attempt has failed but before a new attempt is started
# the application may appear as completed in Spark history. Later on,
# when the new attempt is finished the application will appear as completed
# again but with an updated completion time. As the result, the Spot crawler may
# encounter this application multiple times. To avoid repeated processing,
# the crawler processes only the runs which were completed at least this many seconds ago.
# The default is 900 seconds.
completion_timeout_seconds = 1800

# When processing a particular run causes an exception
# (e.g. missing critical metadata, external services errors, etc.)
# the error message is logged into err_index.
# The setting skip_exceptions defines whether to terminate Crawler process
# or skip the malformed run and continue.
# Skipping is recommended for production deployment,
# while the opposite is recommended for debugging.
skip_exceptions = False

# Data retrieval method
# There are alternative ways how Spot Crawler identifies and retrieves new applications
# from Spark History Server API. The currently implemented methods are:
# - latest: It is based on the assumption that the new apps appear
#           in the chronological order of their completion
#            (which is not guaranteed, especially on a loaded cluster).
#           In each iteration the crawler queries History API
#           for the new apps completed after the latest processed one.
#           This method is faster, but may lead to some runs being missing.
#
# - all:    At each iteration the crawler goes through the entire retention period
#           of the Spark History server (lookback_hours) in steps (time_step_seconds), compares
#           the listed app ids to the ones already stored in the Spot database and processes
#           the newly appeared applications. Such method is slower as it makes more API calls
#           in each iteration but more reliable for a heavily loaded cluster.
#           It requires the following additional configurations:
#                  lookback_hours - retention period of Spark History
#                  time_step_seconds - time interval which is guaranteed
#                             to never have more than 10K jobs completed
# The default method is 'all'
crawler_method=all

lookback_hours = 168
time_step_seconds = 3600

# Certain errors are handled by retrying processing attempts after a pause.
# Currently, such errors include:
# - wrong state of Spark History server when API calls return wrong format. Resolution requires the server restart.
retry_sleep_seconds = 900
retry_attempts = 48

[SPOT_ELASTICSEARCH]
elasticsearch_url = https://localhost:9200


# The following authentication types for Elasticsearch are currently supported:
#   none (leave blank) - no parameters required           (unauthenticated)
#   direct             - username and password required   (authenticated via username & password)
#   cognito            - all parameters required          (authenticated via USER_PASSWORD_AUTH through SRP via Congito)
auth_type =
# Required if using direct/cognito
#username =
#password =
# Required if using cognito
#aws_account_id =
#user_pool_id =
#identity_pool_id =
#cognito_region =
#elasticsearch_region =
#client_id =
#client_secret =
#elasticsearch_role_name =

# INDEXES
# For compatibilty with the provided Kibana dashboards, the Elasticsearch indexes must follow the pattern:
# spot_[raw/agg/err]_<custom postfix>

# The raw_index stores data as received from external APIs.
# It is not fully queryable  in elasticsearch and is needed for debugging purposes only.
# It is recommended NOT to be set in a production configuration, which disables writes to this index.
# raw_index = spot_raw_default

# Agg index contains aggregated data for each Spark Application attempt.
# These data are used in dashboards and analytics. The index is mandatory.
agg_index = spot_agg_default

# The err_index contains a log of internal Spot exceptions for monitoring and debugging purposes.
# Such exceptions may be caused by malformed metadata or external services (Spark history, YARN or Menas) being unavailable.
# The index is mandatory.
err_index = spot_err_default


# By default elasticsearch has a limit of 1000 total fields per index.
# When the value is exceeded Spot incrementally increases the setting.
# By default the increment step is 100.
limit_of_fields_increment = 100

[MISC]
output_dir = output
log_level = debug

[MENAS]
# Menas provides additional metadata for Enceladus jobs (OPTIONAL)
# api_base_url = https://localhost:8080/menas/api
# Provide a path to a CA bundle for Menas HTTPS verification, if required
# menas_ssl_path =
# username = user
# password = changeme

# The default timezone to assume when not provided in datetime stings. The default is UTC when not set.
# For the list of options see:
# https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
# default_timezone = Africa/Johannesburg

[YARN]
# Configs for a separate yarn crawler process

yarn_api_base_url = http://localhost:8088/ws/v1
yarn_clust_index = spot_yarn_cluster_default_1
yarn_apps_index = spot_yarn_apps_default_1
yarn_scheduler_index = spot_yarn_scheduler_default_1
yarn_sleep_seconds = 60
